
@article{goenaga_unmixing_2013,
	title = {Unmixing {Analysis} of a {Time} {Series} of {Hyperion} {Images} {Over} the {Guánica} {Dry} {Forest} in {Puerto} {Rico}},
	volume = {6},
	issn = {1939-1404},
	doi = {10.1109/JSTARS.2012.2225096},
	number = {2},
	journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {Goenaga, M. A. and Torres-Madronero, M. C. and Velez-Reyes, M. and Bloem, S. J. Van and Chinea, J. D.},
	month = apr,
	year = {2013},
	keywords = {abundance map variations, AD 2008, Correlation, Dry forest, endmember extraction, EO-1, EO-1 satellite, extracted spectral endmembers, forestry, geophysical image processing, Guanica dry forest, Hyperion, Hyperion image time series, Hyperion sensor, hyperspectral images, hyperspectral imaging, Hyperspectral imaging, hyperspectral scene, image sequences, image unmixing analysis, linear unmixing, local endmember signatures, local unmixing procedure, mangrove, NDVI time series, near cloud free Hyperion images, Puerto Rico, rainfall time series, remote sensing, single spectral signature, time series, Time series analysis, vegetation cover seasonal variations, vegetation mapping, Vegetation mapping},
	pages = {329--338}
}

@techreport{nvidia_corp._nvidia_2017,
	type = {White {Paper}},
	title = {{NVIDIA} {TESLA} {V100} {GPU} {ARCHITECTURE}},
	url = {http://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf},
	number = {WP-08608-001\_v1.1},
	author = {NVidia Corp.},
	month = aug,
	year = {2017}
}

@inproceedings{reyes_change_2017,
	title = {Change detection assessment in a tropical forest using multispectral and hyperspectral images},
	doi = {10.1109/IGARSS.2017.8127031},
	booktitle = {2017 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium} ({IGARSS})},
	author = {Reyes, E. and Abad, R. J. and Diaz, J. and Manian, V.},
	month = jul,
	year = {2017},
	keywords = {geophysical image processing, hyperspectral images, hyperspectral imaging, Hyperspectral imaging, remote sensing, Vegetation mapping, Artificial satellites, change detection assessment, change detection method, Earth, Euclidean distance, Euclidean metric, image classification, Image resolution, land cover, land cover changes, Landsat 7 multispectral images, Landsat 8 multispectral images, multispectral images, tasseled cap transformation, Tasseled Cap Transformation, terrain mapping, time 10.0 year, tropical dry forest area},
	pages = {624--627}
}

@article{liu_sparse_2017,
	title = {Sparse {Tensor}-{Based} {Dimensionality} {Reduction} for {Hyperspectral} {Spectral} \#x2013;{Spatial} {Discriminant} {Feature} {Extraction}},
	volume = {14},
	issn = {1545-598X},
	doi = {10.1109/LGRS.2017.2734960},
	abstract = {This letter explores a spectral-spatial tensor-based dimensionality reduction (DR) method to cope with hyperspectral image (HSI) feature extraction and classification. This method uses the Gabor filter banks as the bias spectral-spatial feature hybrider and further integrates the tensor-based alignment strategy for the discriminant locality with sparse factorization by extracting optimal spectral-spatial features and simultaneously maintaining structural relevance. Comparative experimental results with two real HSIs demonstrate that the proposed DR method has a considerable advantage over other traditional feature extraction methods.},
	number = {10},
	journal = {IEEE Geoscience and Remote Sensing Letters},
	author = {Liu, Z. and Tang, B. and He, X. and Qiu, Q. and Wang, H.},
	month = oct,
	year = {2017},
	keywords = {geophysical image processing, Hyperspectral imaging, Dimensionality reduction (DR), Feature extraction, Gabor filter banks, HSIs, hyperspectral image (HSI), hyperspectral spectral-spatial discriminant feature extraction, Principal component analysis, sparse factorization, Sparse matrices, sparse tensor-based dimensionality reduction, spectral–spatial feature extraction, Tensile stress, tensor discriminant analysis, traditional feature extraction methods},
	pages = {1775--1779}
}

@article{gupta_processing_2015,
	title = {Processing {Hyperspectral} {Images} using {Non}-{Linear} {Least} {Square} {Algorithm} as an {Optimization} {Method} for {Tensor} {Decomposition} {Model}},
	volume = {123},
	number = {12},
	journal = {International Journal of Computer Applications},
	author = {Gupta, Ankit and Goel, Nishi and Oberoi, Ashish},
	month = aug,
	year = {2015},
	pages = {14--19},
	annote = {Published by Foundation of Computer Science (FCS), NY, USA}
}

@article{qian_matrix-vector_2017,
	title = {Matrix-{Vector} {Nonnegative} {Tensor} {Factorization} for {Blind} {Unmixing} of {Hyperspectral} {Imagery}},
	volume = {55},
	issn = {0196-2892},
	doi = {10.1109/TGRS.2016.2633279},
	abstract = {Many spectral unmixing approaches ranging from geometry, algebra to statistics have been proposed, in which nonnegative matrix factorization (NMF)-based ones form an important family. The original NMF-based unmixing algorithm loses the spectral and spatial information between mixed pixels when stacking the spectral responses of the pixels into an observed matrix. Therefore, various constrained NMF methods are developed to impose spectral structure, spatial structure, and spectral-spatial joint structure into NMF to enforce the estimated endmembers and abundances preserve these structures. Compared with matrix format, the third-order tensor is more natural to represent a hyperspectral data cube as a whole, by which the intrinsic structure of hyperspectral imagery can be losslessly retained. Extended from NMF-based methods, a matrix-vector nonnegative tensor factorization (NTF) model is proposed in this paper for spectral unmixing. Different from widely used tensor factorization models, such as canonical polyadic decomposition CPD) and Tucker decomposition, the proposed method is derived from block term decomposition, which is a combination of CPD and Tucker decomposition. This leads to a more flexible frame to model various application-dependent problems. The matrix-vector NTF decomposes a third-order tensor into the sum of several component tensors, with each component tensor being the outer product of a vector (endmember) and a matrix (corresponding abundances). From a formal perspective, this tensor decomposition is consistent with linear spectral mixture model. From an informative perspective, the structures within spatial domain, within spectral domain, and cross spectral-spatial domain are retreated interdependently. Experiments demonstrate that the proposed method has outperformed several state-of-the-art NMF-based unmixing methods.},
	number = {3},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Qian, Y. and Xiong, F. and Zeng, S. and Zhou, J. and Tang, Y. Y.},
	month = mar,
	year = {2017},
	keywords = {hyperspectral imaging, Hyperspectral imaging, Feature extraction, Tensile stress, application-dependent problems, block term decomposition, canonical polyadic decomposition, component tensor, CPD, cross spectral-spatial domain, Distance measurement, hyperspectral data cube, Hyperspectral imagery (HSI), hyperspectral imagery blind unmixing, image processing, linear spectral mixture model, matrix decomposition, Matrix decomposition, matrix-vector nonnegative tensor factorization, Mixture models, NMF-based unmixing algorithm, NTF model, spatial information, spatial structure, spectral information, spectral response stacking, spectral structure, spectral unmixing, spectral unmixing approaches, spectral-spatial joint structure, spectral-spatial structure, tensor decomposition, tensor factorization, tensors, third-order tensor, Tucker decomposition},
	pages = {1776--1792}
}

@article{wu_parallel_2016,
	title = {Parallel and {Distributed} {Dimensionality} {Reduction} of {Hyperspectral} {Data} on {Cloud} {Computing} {Architectures}},
	volume = {9},
	issn = {1939-1404},
	doi = {10.1109/JSTARS.2016.2542193},
	abstract = {Cloud computing offers the possibility to store and process massive amounts of remotely sensed hyperspectral data in a distributed way. Dimensionality reduction is an important task in hyperspectral imaging, as hyperspectral data often contains redundancy that can be removed prior to analysis of the data in repositories. In this regard, the development of dimensionality reduction techniques in cloud computing environments can provide both efficient storage and preprocessing of the data. In this paper, we develop a parallel and distributed implementation of a widely used technique for hyperspectral dimensionality reduction: principal component analysis (PCA), based on cloud computing architectures. Our implementation utilizes Hadoop's distributed file system (HDFS) to realize distributed storage, uses Apache Spark as the computing engine, and is developed based on the map-reduce parallel model, taking full advantage of the high throughput access and high performance distributed computing capabilities of cloud computing environments. We first optimized the traditional PCA algorithm to be well suited for parallel and distributed computing, and then we implemented it on a real cloud computing architecture. Our experimental results, conducted using several hyperspectral datasets, reveal very high performance for the proposed distributed parallel method.},
	number = {6},
	journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {Wu, Z. and Li, Y. and Plaza, A. and Li, J. and Xiao, F. and Wei, Z.},
	month = jun,
	year = {2016},
	keywords = {hyperspectral imaging, Hyperspectral imaging, remote sensing, Principal component analysis, Apache spark, cloud computing, Cloud computing, cloud computing architecture, cloud computing environment, Computer architecture, computing engine, dimensionality reduction, distributed computing, Distributed databases, distributed dimensionality reduction, distributed parallel method, distributed storage, geophysical techniques, geophysics computing, Hadoop, Hadoop distributed file system, hyperspectral data, hyperspectral dataset, hyperspectral dimensionality reduction, map-reduce parallel model, parallel algorithms, parallel architectures, parallel computing, parallel dimensionality, parallel processing, PCA, principal component analysis, principal component analysis (PCA), remotely sensed hyperspectral data, spark, sparks, Sparks, traditional PCA algorithm},
	pages = {2270--2278}
}

@inproceedings{smith_sparse_2017,
	title = {Sparse {Tensor} {Factorization} on {Many}-{Core} {Processors} with {High}-{Bandwidth} {Memory}},
	doi = {10.1109/IPDPS.2017.84},
	abstract = {HPC systems are increasingly used for data intensive computations which exhibit irregular memory accesses, non-uniform work distributions, large memory footprints, and high memory bandwidth demands. To address these challenging demands, HPC systems are turning to many-core architectures that feature a large number of energy-efficient cores backed by high-bandwidth memory. These features are exemplified in Intel's recent Knights Landing many-core processor (KNL), which typically has 68 cores and 16GB of on-package multi-channel DRAM (MCDRAM). This work investigates how the novel architectural features offered by KNL can be used in the context of decomposing sparse, unstructured tensors using the canonical polyadic decomposition (CPD). The CPD is used extensively to analyze large multi-way datasets arising in various areas including precision healthcare, cybersecurity, and e-commerce. Towards this end, we (i) develop problem decompositions for the CPD which are amenable to hundreds of concurrent threads while maintaining load balance and low synchronization costs; and (ii) explore the utilization of architectural features such as MCDRAM. Using one KNL processor, our algorithm achieves up to 1.8x speedup over a dual socket Intel Xeon system with 44 cores.},
	booktitle = {2017 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} ({IPDPS})},
	author = {Smith, S. and Park, J. and Karypis, G.},
	month = may,
	year = {2017},
	keywords = {Sparse matrices, Tensile stress, canonical polyadic decomposition, CPD, matrix decomposition, tensors, Bandwidth, cybersecurity, data intensive computations, dual socket Intel Xeon system, e-commerce, energy-efficient cores, high memory bandwidth demands, high-bandwidth memory, HPC systems, Instruction sets, irregular memory accesses, knights landing many-core processor, KNL processor, large memory footprints, load balance, many-core architectures, many-core processors, MCDRAM, microprocessor chips, multiway datasets, nonuniform work distributions, parallel machines, Parallel processing, precision healthcare, resource allocation, sparse tensor factorization, Synchronization, synchronization costs, unstructured tensors},
	pages = {1058--1067}
}

@article{kannan_mpi-faun:_2018,
	title = {{MPI}-{FAUN}: {An} {MPI}-{Based} {Framework} for {Alternating}-{Updating} {Nonnegative} {Matrix} {Factorization}},
	volume = {30},
	issn = {1041-4347},
	doi = {10.1109/TKDE.2017.2767592},
	abstract = {Non-negative matrix factorization (NMF) is the problem of determining two non-negative low rank factors Wand H, for the given input matrix A, such that A WH. NMF is a useful tool for many applications in different domains such as topic modeling in text mining, background separation in video analysis, and community detection in social networks. Despite its popularity in the data mining community, there is a lack of efficient parallel algorithms to solve the problem for big data sets. The main contribution of this work is a new, high-performance parallel computational framework for a broad class of NMF algorithms that iteratively solves alternating non-negative least squares (NLS) subproblems for W and H. It maintains the data and factor matrices in memory (distributed across processors), uses MPI for interprocessor communication, and, in the dense case, provably minimizes communication costs (under mild assumptions). The framework is flexible and able to leverage a variety of NMF and NLS algorithms, including Multiplicative Update, Hierarchical Alternating Least Squares, and Block Principal Pivoting. Our implementation allows us to benchmark and compare different algorithms on massive dense and sparse data matrices of size that spans from few hundreds of millions to billions. We demonstrate the scalability of our algorithm and compare it with baseline implementations, showing significant performance improvements. The code and the datasets used for conducting the experiments are available online.},
	number = {3},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Kannan, R. and Ballard, G. and Park, H.},
	month = mar,
	year = {2018},
	keywords = {Sparse matrices, matrix decomposition, parallel algorithms, 2D, Algorithm design and analysis, Analytical models, Approximation algorithms, Big Data, big data sets, Block Principal Pivoting, Collaboration, communication cost minimization, Computational modeling, data mining community, factor matrices, given input matrix, Hierarchical Alternating Least Squares, high-performance parallel computational framework, HPC, interprocessor communication, iterative methods, least squares approximations, matrix algebra, message passing, MPI, MPI-FAUN, Multiplicative Update, NLS algorithms, NMF, NMF algorithms, nonnegative least squares subproblems, nonnegative matrix factorization, Program processors},
	pages = {544--558}
}

@inproceedings{hu_towards_2015,
	title = {Towards an {Efficient} {Multi}-way {Factorization} of {Multi}-dimensional {Big} {Data} across a {GPU} {Cluster}},
	doi = {10.1109/DS-RT.2015.17},
	abstract = {It has long been an important issue in various disciplines to examine massive multi-dimensional data by extracting the embedded multi-way factors. With the quick increases in both scales and dimensions of data under analysis, research challenges arise in order to reflect the dynamics of large-scale tensors while introducing no significant distortions in the factorization procedure in sophisticated applications. A massively parallel computing framework, namely H-PARAFAC, has been developed to enable Parallel Factor Analysis (PARAFAC) of massive tensors upon a "divide-and-conquer" theory (a modified alternating least squares approach). The hierarchical framework incorporates a coarse-grained model for coordinating the processing of sub tensors and a fine-grained parallel model for computing each sub tensor and fusing sub-factors. Experiments have been performed on a GPU cluster, and the results indicate that (1) the proposed method breaks the limitation on the size of data to be factorized, and (2) it dramatically outperforms the traditional counterparts in terms of both scalability and efficiency, e.g., The runtime increases linearly with the data volume increases in the order of n3.},
	booktitle = {2015 {IEEE}/{ACM} 19th {International} {Symposium} on {Distributed} {Simulation} and {Real} {Time} {Applications} ({DS}-{RT})},
	author = {Hu, Y. and Wang, L. and Liu, Y. and Chen, D. and Li, X.},
	month = oct,
	year = {2015},
	keywords = {Tensile stress, tensors, parallel processing, Parallel processing, Big Data, Computational modeling, least squares approximations, coarse-grained model, divide and conquer methods, divide-and-conquer theory, embedded multiway factor extraction, Factorization, fine-grained parallel model, GPU cluster, graphics processing units, Graphics processing units, hierarchical framework, large-scale tensor dynamics, massively parallel computing framework, Mathematical model, modified alternating least squares approach, Multi-dimensional Data Processing, multidimensional Big Data, multiway factorization, Parallel Computing, parallel factor analysis, Parallel Factor Analysis, Real-time Application, Scalability, Yttrium},
	pages = {18--24}
}

@article{guerra_evaluation_2017,
	title = {On the {Evaluation} of {Different} {High}-{Performance} {Computing} {Platforms} for {Hyperspectral} {Imaging}: {An} {OpenCL}-{Based} {Approach}},
	volume = {10},
	issn = {1939-1404},
	doi = {10.1109/JSTARS.2017.2737958},
	abstract = {Hyperspectral imaging systems are a powerful tool for obtaining surface information in many different spectral channels that can be used in many different applications. Nevertheless, the huge amount of information provided by hyperspectral images also has a downside, since it has to be processed and analyzed. For such purpose, parallel hardware devices, such as field-programmable gate arrays (FPGAs) and graphic processing units (GPUs), are typically used, especially for hyperspectral imaging applications under real-time constraints. However, developing hardware applications typically requires expertise in the specific targeted device, as well as in the tools and methodologies that can be used to perform the implementation of the desired algorithms in that device. In this scenario, the Open Computing Language (OpenCL) emerges as a very interesting solution in which a single high-level language can be used to efficiently develop applications in multiple and different hardware devices. In this work, the parallel Fast Algorithm for Linearly Unmixing Hyperspectral Images (pFUN) has been implemented in two different NVIDIA GPUs, the GeForce GTX 980 and the Tesla K40c, using OpenCL. The obtained results are compared with the results provided by the previously developed NVIDIA CUDA implementation of the pFUN algorithm for the same GPU devices for comparing the efficiency of OpenCL against a more specific synthesis design language for the targeted hardware devices, such as CUDA is for NVIDIA GPUs. Moreover, the FUN algorithm has also been implemented into a Bitware Stratix V Altera FPGA, using OpenCL, for comparing the results that can be obtained using OpenCL when targeting different devices and architectures. The obtained results demonstrate the suitability of the followed methodology in the sense that it allows the achievement of efficient FPGA and GPU implementations able to cope with the stringent requirements imposed by hyperspectral imaging systems.},
	number = {11},
	journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {Guerra, R. and Martel, E. and Khan, J. and López, S. and Athanas, P. and Sarmiento, R.},
	month = nov,
	year = {2017},
	keywords = {hyperspectral imaging, Hyperspectral imaging, Graphics processing units, CUDA, Field programmable gate arrays, field-programmable gate array (FPGA), graphic processing unit (GPU), Hardware, high-performance computing, hyperspectral unmixing, open computing language (openCL), parallel programing, Performance evaluation, Real-time systems},
	pages = {4879--4897}
}

@book{landgrebe_signal_2003,
	title = {Signal {Theory} {Methods} in {Multispectral} {Remote} {Sensing}},
	publisher = {Wiley},
	author = {Landgrebe, David A.},
	year = {2003},
	keywords = {APPLICATION remote sensing},
	annote = {SIGNATUR = 785.173}
}

@misc{jpl_aviris_nodate,
	title = {{AVIRIS} {Overview}},
	url = {https://aviris.jpl.nasa.gov/aviris/index.html},
	journal = {AVIRIS},
	author = {JPL}
}

@article{keshava_spectral_2002,
	title = {Spectral unmixing},
	volume = {19},
	issn = {1053-5888},
	doi = {10.1109/79.974727},
	abstract = {Spectral unmixing using hyperspectral data represents a significant step in the evolution of remote decompositional analysis that began with multispectral sensing. It is a consequence of collecting data in greater and greater quantities and the desire to extract more detailed information about the material composition of surfaces. Linear mixing is the key assumption that has permitted well-known algorithms to be adapted to the unmixing problem. In fact, the resemblance of the linear mixing model to system models in other areas has permitted a significant legacy of algorithms from a wide range of applications to be adapted to unmixing. However, it is still unclear whether the assumption of linearity is sufficient to model the mixing process in every application of interest. It is clear, however, that the applicability of models and techniques is highly dependent on the variety of circumstances and factors that give rise to mixed pixels. The outputs of spectral unmixing, endmember, and abundance estimates are important for identifying the material composition of mixtures},
	number = {1},
	journal = {IEEE Signal Processing Magazine},
	author = {Keshava, N. and Mustard, J. F.},
	month = jan,
	year = {2002},
	keywords = {Hyperspectral imaging, remote sensing, spectral unmixing, hyperspectral data, PCA, principal component analysis, Analytical models, abundance estimates, Atmospheric modeling, Data mining, dimension reduction, endmember determination, Hyperspectral sensors, Layout, linear mixing model, material composition, multispectral sensing, nonlinear mixing, nonstatistical methods, Reflectivity, remote decompositional analysis, Remote sensing, Spatial resolution, spectral analysis, Spectroscopy},
	pages = {44--57}
}

@inproceedings{lee_algorithms_2000,
	title = {Algorithms for {Non}-negative {Matrix} {Factorization}},
	booktitle = {In {NIPS}},
	publisher = {MIT Press},
	author = {Lee, Daniel D. and Seung, H. Sebastian},
	year = {2000},
	pages = {556--562}
}

@article{zhou_nonnegative_2014,
	title = {Nonnegative {Matrix} and {Tensor} {Factorizations} : {An} algorithmic perspective},
	volume = {31},
	issn = {1053-5888},
	doi = {10.1109/MSP.2014.2298891},
	abstract = {A common thread in various approaches for model reduction, clustering, feature extraction, classification, and blind source separation (BSS) is to represent the original data by a lower-dimensional approximation obtained via matrix or tensor (multiway array) factorizations or decompositions. The notion of matrix/tensor factorizations arises in a wide range of important applications and each matrix/tensor factorization makes different assumptions regarding component (factor) matrices and their underlying structures. So choosing the appropriate one is critical in each application domain. Approximate low-rank matrix and tensor factorizations play fundamental roles in enhancing the data and extracting latent (hidden) components.},
	number = {3},
	journal = {IEEE Signal Processing Magazine},
	author = {Zhou, G. and Cichocki, A. and Zhao, Q. and Xie, S.},
	month = may,
	year = {2014},
	keywords = {Feature extraction, Sparse matrices, Tensile stress, matrix decomposition, Matrix decomposition, tensor factorization, tensors, Approximation methods, approximation theory, blind source separation, Clustering, data enhancement, feature extraction, latent component extraction, model reduction, nonnegative matrix, pattern classification, pattern clustering, rank matrix approximation, Signal processing algorithms, Source separation, statistical analysis},
	pages = {54--65}
}

@incollection{donoho_when_2004,
	title = {When {Does} {Non}-{Negative} {Matrix} {Factorization} {Give} a {Correct} {Decomposition} into {Parts}?},
	url = {http://papers.nips.cc/paper/2463-when-does-non-negative-matrix-factorization-give-a-correct-decomposition-into-parts.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 16},
	publisher = {MIT Press},
	author = {Donoho, David and Stodden, Victoria},
	editor = {Thrun, S. and Saul, L. K. and Schölkopf, B.},
	year = {2004},
	pages = {1141--1148}
}

@incollection{kruskal_multiway_1989,
	address = {Amsterdam, The Netherlands, The Netherlands},
	title = {Multiway {Data} {Analysis}},
	isbn = {0-444-87410-0},
	url = {http://dl.acm.org/citation.cfm?id=120565.120567},
	publisher = {North-Holland Publishing Co.},
	author = {Kruskal, J. B.},
	editor = {Coppi, R. and Bolasco, S.},
	year = {1989},
	pages = {7--18}
}

@book{sidiropoulos_uniqueness_2000,
	title = {On the {Uniqueness} of {Multilinear} {Decomposition} of {N}-way arrays},
	author = {Sidiropoulos, Nicholas D. and Bro, Rasmus},
	year = {2000}
}

@article{tucker_mathematical_1966,
	title = {Some mathematical notes on three-mode factor analysis},
	volume = {31},
	doi = {10.1007/BF02289464},
	journal = {Psychometrika},
	author = {Tucker, L. R.},
	year = {1966},
	pages = {279--311}
}

@article{caroll_analysis_1970,
	title = {Analysis of individual differences in multidimensional scaling via n-way generalization of {Eckart}–{Young} decomposition},
	volume = {35},
	doi = {10.1007/BF02310791},
	journal = {Psychometrika},
	author = {Caroll, J. D. and Chang, J. J},
	year = {1970},
	pages = {283--319}
}

@article{sidiropoulos_parallel_2000,
	title = {Parallel factor analysis in sensor array processing},
	volume = {48},
	number = {8},
	journal = {IEEE transactions on Signal Processing},
	author = {Sidiropoulos, N.D. and Bro, R. and Giannakis, G.B.},
	year = {2000},
	pages = {2377--2388}
}

@article{huang_non-negative_2014,
	title = {Non-{Negative} {Matrix} {Factorization} {Revisited}: {Uniqueness} and {Algorithm} for {Symmetric} {Decomposition}},
	volume = {62},
	issn = {1053-587X},
	doi = {10.1109/TSP.2013.2285514},
	abstract = {Non-negative matrix factorization (NMF) has found numerous applications, due to its ability to provide interpretable decompositions. Perhaps surprisingly, existing results regarding its uniqueness properties are rather limited, and there is much room for improvement in terms of algorithms as well. Uniqueness aspects of NMF are revisited here from a geometrical point of view. Both symmetric and asymmetric NMF are considered, the former being tantamount to element-wise non-negative square-root factorization of positive semidefinite matrices. New uniqueness results are derived, e.g., it is shown that a sufficient condition for uniqueness is that the conic hull of the latent factors is a superset of a particular second-order cone. Checking this condition is shown to be NP-complete; yet this and other results offer insights on the role of latent sparsity in this context. On the computational side, a new algorithm for symmetric NMF is proposed, which is very different from existing ones. It alternates between Procrustes rotation and projection onto the non-negative orthant to find a non-negative matrix close to the span of the dominant subspace. Simulation results show promising performance with respect to the state-of-art. Finally, the new algorithm is applied to a clustering problem for co-authorship data, yielding meaningful and interpretable results.},
	number = {1},
	journal = {IEEE Transactions on Signal Processing},
	author = {Huang, K. and Sidiropoulos, N. D. and Swami, A.},
	month = jan,
	year = {2014},
	keywords = {matrix decomposition, Matrix decomposition, nonnegative matrix factorization, Signal processing algorithms, statistical analysis, asymmetric NMF, Clustering algorithms, clustering problem, coauthorship data, conic hull, dominant subspace, element wise nonnegative square root factorization, Linear matrix inequalities, Nonnegative matrix factorization (NMF), NP-complete problem, optimisation, Procrustes projection, Procrustes rotation, Prototypes, semidefinite matrix, sparse latent factor, sparsity, sufficient condition, symmetric decomposition, Symmetric matrices, symmetric NMF, uniqueness, uniqueness property, Vectors},
	pages = {211--224}
}

@article{kim_algorithms_2014,
	title = {Algorithms for {Nonnegative} {Matrix} and {Tensor} {Factorizations}: {A} {Unified} {View} {Based} on {Block} {Coordinate} {Descent} {Framework}},
	volume = {58},
	issn = {0925-5001},
	url = {http://dx.doi.org/10.1007/s10898-013-0035-4},
	doi = {10.1007/s10898-013-0035-4},
	number = {2},
	journal = {J. of Global Optimization},
	author = {Kim, Jingu and He, Yunlong and Park, Haesun},
	month = feb,
	year = {2014},
	keywords = {Block coordinate descent, Low-rank approximation, Nonnegative matrix factorization, Nonnegative tensor factorization},
	pages = {285--319}
}

@article{sidiropoulos_tensor_2017,
	title = {Tensor {Decomposition} for {Signal} {Processing} and {Machine} {Learning}},
	volume = {65},
	issn = {1053-587X},
	doi = {10.1109/TSP.2017.2690524},
	abstract = {Tensors or multiway arrays are functions of three or more indices (i, j, k, . . . )-similar to matrices (two-way arrays), which are functions of two indices (r, c) for (row, column). Tensors have a rich history, stretching over almost a century, and touching upon numerous disciplines; but they have only recently become ubiquitous in signal and data analytics at the confluence of signal processing, statistics, data mining, and machine learning. This overview article aims to provide a good starting point for researchers and practitioners interested in learning about and working with tensors. As such, it focuses on fundamentals and motivation (using various application examples), aiming to strike an appropriate balance of breadth and depth that will enable someone having taken first graduate courses in matrix algebra and probability to get started doing research and/or developing tensor algorithms and software. Some background in applied optimization is useful but not strictly required. The material covered includes tensor rank and rank decomposition; basic tensor factorization models and their relationships and properties (including fairly good coverage of identifiability); broad coverage of algorithms ranging from alternating optimization to stochastic gradient; statistical performance analysis; and applications ranging from source separation to collaborative filtering, mixture and topic modeling, classification, and multilinear subspace learning.},
	number = {13},
	journal = {IEEE Transactions on Signal Processing},
	author = {Sidiropoulos, N. D. and Lathauwer, L. De and Fu, X. and Huang, K. and Papalexakis, E. E. and Faloutsos, C.},
	month = jul,
	year = {2017},
	keywords = {Tensile stress, matrix decomposition, Matrix decomposition, tensor decomposition, tensor factorization, tensors, matrix algebra, Signal processing algorithms, uniqueness, alternating direction method of multipliers, alternating optimization, canonical polyadic decomposition (CPD), classification, collaborative filtering, communications, Cramér–Rao bound, data analytics, Gauss–Newton, gradient descent, harmonic retrieval, higher-order singular value decomposition (HOSVD), learning (artificial intelligence), machine learning, mixture modeling, multilinear singular value decomposition (MLSVD), multilinear subspace learning, multiway arrays, NP-hard problems, Optimization, parallel factor analysis (PARAFAC), rank, rank decomposition, signal analytics, signal processing, Signal processing, source separation, speech separation, stochastic gradient, subspace learning, Tensor decomposition, tensor rank, topic modeling, Tucker model, Tutorials},
	pages = {3551--3582}
}

@article{veganzones_nonnegative_2016,
	title = {Nonnegative {Tensor} {CP} {Decomposition} of {Hyperspectral} {Data}},
	volume = {54},
	issn = {0196-2892},
	doi = {10.1109/TGRS.2015.2503737},
	abstract = {New hyperspectral missions will collect huge amounts of hyperspectral data. In addition, it is possible now to acquire time series and multiangular hyperspectral images. The process and analysis of these big data collections will require common hyperspectral techniques to be adapted or reformulated. The tensor decomposition, which is also known as multiway analysis, is a technique to decompose multiway arrays, i.e., hypermatrices with more than two dimensions (ways). Hyperspectral time series and multiangular acquisitions can be represented as a three-way tensor. Here, we apply canonical polyadic (CP) tensor decomposition techniques to the blind analysis ohyperspectral big data. In order to do so, we use a novel compression-based nonnegative CP decomposition. We show that the proposed methodology can be interpreted as multilinear blind spectral unmixing, i.e., a higher order extension of the widely known spectral unmixing. In the proposed approach, the big hyperspectral tensor is decomposed in three sets of factors, which can be interpreted as spectral signatures, their spatial distribution, and temporal/angular changes. We provide experimental validation using a study case of the snow coverage of the French Alps during the snow season.},
	number = {5},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Veganzones, M. A. and Cohen, J. E. and Farias, R. Cabral and Chanussot, J. and Comon, P.},
	month = may,
	year = {2016},
	keywords = {hyperspectral imaging, Hyperspectral imaging, remote sensing, time series, Time series analysis, Tensile stress, Matrix decomposition, tensors, hyperspectral data, Approximation methods, Big data, canonical polyadic (CP) decomposition, compression, French Alps, hyperspectral, hyperspectral tensor, multilinear blind spectral unmixing, nonnegative tensor CP decomposition, nonnegative tensor decomposition, polyadic tensor decomposition techniques, snow season, spatial distribution},
	pages = {2577--2588}
}

@article{kolda_tensor_2009,
	title = {Tensor {Decompositions} and {Applications}},
	volume = {51},
	number = {3},
	journal = {SIAM REVIEW},
	author = {Kolda, Tamara G. and Bader, Brett W.},
	year = {2009},
	pages = {455--500}
}

@article{kiers_relating_1997,
	title = {Relating two proposed methods for speedup of algorithms for fitting two- and three-way principal component and related multilinear models},
	volume = {36},
	issn = {0169-7439},
	url = {http://www.sciencedirect.com/science/article/pii/S0169743996000743},
	doi = {https://doi.org/10.1016/S0169-7439(96)00074-3},
	abstract = {Multilinear analysis methods such as component (and three-way component) analysis of very large data sets can become very computationally demanding and even infeasible unless some method is used to compress the data and/or speed up the algorithms. We discuss two previously proposed speedup methods. (a) Alsberg and Kvalheim have proposed use of data simplification along with some new analysis algorithms. We show that their procedures solve the same problem as (b) the more general approach proposed (in a different context) by Carroll, Pruzansky, and Kruskal. In the latter approach, a speed improvement is attained by applying any (three-mode) PCA algorithm to a small (three-way) array derived from the original data. Hence, it can employ the new algorithms by Alsberg and Kvalheim, but, as is shown in the present paper, it is easier and often more efficient to apply standard (three-mode) PCA algorithms to the small array. Finally, it is shown how the latter approach for speed improvement can also be used for other three-way models and analysis methods (e.g., PARAFAC/CANDECOMP and constrained three-mode PCA).},
	number = {1},
	journal = {Chemometrics and Intelligent Laboratory Systems},
	author = {Kiers, Henk A. L. and Harshman, Richard A.},
	year = {1997},
	keywords = {Principal component analysis, Multilinear models, Two- and three-way principal component model},
	pages = {31 -- 40}
}

@article{wu_sparse_2014,
	title = {Sparse {Non}-negative {Matrix} {Factorization} on {GPUs} for {Hyperspectral} {Unmixing}},
	volume = {7},
	issn = {1939-1404},
	doi = {10.1109/JSTARS.2014.2315045},
	abstract = {Hyperspectral unmixing is a typical problem of blind source separation, which can be solved by non-negative matrix factorization (NMF). NMF based on sparsity, which can increase the efficiency of unmixing, is an important topic in hyperspectral unmixing. In this paper, a novel constrained sparse (CS) NMF (CSNMF) method for hyperspectral unmixing is proposed, where a new sparsity term is introduced to improve the stability and accuracy of unmixing model. The corresponding algorithm is designed based on the alternating direction method of multiplies. In order to further enhance the execution speed, parallel optimization of hyperspectral unmixing based on CSNMF on graphics processing units (CSNMF-GPU) is investigated and compared in terms of both accuracy and speed. The realization of the proposed method using compute unified device architecture (CUDA) on GPUs is described and evaluated. Experimental results based on the simulated hyperspectral datasets show that the proposed CSNMF method can improve the unmixing accuracy steadily. The tests comparing the parallel optimization of CSNMF on GPUs with the serial implementation and the multicore implementation, using both simulated and real hyperspectral data, demonstrate the effectiveness of the CSNMF-GPU approach.},
	number = {8},
	journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {Wu, Z. and Ye, S. and Liu, J. and Sun, L. and Wei, Z.},
	month = aug,
	year = {2014},
	keywords = {Hyperspectral imaging, Sparse matrices, geophysical techniques, geophysics computing, parallel architectures, Instruction sets, graphics processing units, Graphics processing units, hyperspectral unmixing, blind source separation, sparsity, Optimization, hyperspectral, Accuracy, compute unified device architecture, CSNMF method, CSNMF parallel optimization, CSNMF-GPU approach, Graphics processing units (GPUs), hyperspectral datasets, hyperspectral remote sensors, non-negative matrix factorization (NMF), novel constrained sparse, parallel optimization, sparse nonnegative matrix factorization, unmixing, unmixing model},
	pages = {3640--3649}
}

@inproceedings{liu_unified_2017,
	title = {A {Unified} {Optimization} {Approach} for {Sparse} {Tensor} {Operations} on {GPUs}},
	doi = {10.1109/CLUSTER.2017.75},
	abstract = {Sparse tensors appear in many large-scale applications with multidimensional and sparse data. While multidimensional sparse data often need to be processed on manycore processors, attempts to develop highly-optimized GPU-based implementations of sparse tensor operations are rare. The irregular computation patterns and sparsity structures as well as the large memory footprints of sparse tensor operations make such implementations challenging. We leverage the fact that sparse tensor operations share similar computation patterns to propose a unified tensor representation called F-COO. Combined with GPU-specific optimizations, F-COO provides highly-optimized implementations of sparse tensor computations on GPUs. The performance of the proposed unified approach is demonstrated for tensor-based kernels such as the Sparse Matricized Tensor-Times-Khatri-Rao Product (SpMTTKRP) and the Sparse Tensor-Times-Matrix Multiply (SpTTM) and is used in tensor decomposition algorithms. Compared to state-of-the-art work we improve the performance of SpTTM and SpMTTKRP up to 3.7 and 30.6 times respectively on NVIDIA Titan-X GPUs. We implement a CANDECOMP/PARAFAC (CP) decomposition and achieve up to 14.9 times speedup using the unified method over state-of-the-art libraries on NVIDIA Titan-X GPUs.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Cluster} {Computing} ({CLUSTER})},
	author = {Liu, B. and Wen, C. and Sarwate, A. D. and Dehnavi, M. M.},
	month = sep,
	year = {2017},
	keywords = {Sparse matrices, Tensile stress, Matrix decomposition, tensors, graphics processing units, Graphics processing units, Parallel Computing, Optimization, Acceleration, GPU, mathematics computing, matrix multiplication, multidimensional data, multiprocessing systems, NVIDIA Titan-X GPU, Parallel algorithms, sparse data, sparse matrices, Sparse Matricized Tensor-Times-Khatri-Rao Product, sparse tensor computations, sparse tensor operations, Sparse Tensor Operations, Sparse Tensor-Times-Matrix Multiply, SpMTTKRP, SpTTM, tensor decomposition algorithms, tensor-based kernels, Unified Apprpach, unified tensor representation},
	pages = {47--57}
}

@article{sorber_optimization-based_2013,
	title = {Optimization-{Based} {Algorithms} for {Tensor} {Decompositions}: {Canonical} {Polyadic} {Decomposition}, {Decomposition} in {Rank}-\$({L}\_r,{L}\_r,1)\$ {Terms}, and a {New} {Generalization}},
	volume = {23},
	url = {https://doi.org/10.1137/120868323},
	doi = {10.1137/120868323},
	number = {2},
	journal = {SIAM Journal on Optimization},
	author = {Sorber, Laurent. and Van Barel, Marc. and De Lathauwer, Lieven.},
	year = {2013},
	pages = {695--720}
}

@inproceedings{lathauwer_block_2012,
	title = {Block {Component} {Analysis}, a {New} {Concept} for {Blind} {Source} {Separation}},
	volume = {7191},
	doi = {10.1007/978-3-642-28551-6_1},
	author = {Lathauwer, Lieven},
	year = {2012},
	pages = {1--8}
}

@article{heinz_fully_2001,
	title = {Fully constrained least squares linear spectral mixture analysis method for material quantification in hyperspectral imagery},
	volume = {39},
	doi = {10.1109/36.911111},
	abstract = {Linear spectral mixture analysis (LSMA) is a widely used technique in remote sensing to estimate abundance fractions of materials present in an image pixel. In order for an LSMA-based estimator to produce accurate amounts of material abundance, it generally requires two constraints imposed on the linear mixture model used in LSMA, which are the abundance sum-to-one constraint and the abundance nonnegativity constraint. The first constraint requires the sum of the abundance fractions of materials present in an image pixel to be one and the second imposes a constraint that these abundance fractions be nonnegative. While the first constraint is easy to deal with, the second constraint is difficult to implement since it results in a set of inequalities and can only be solved by numerical methods. Consequently, most LSMA-based methods are unconstrained and produce solutions that do not necessarily reflect the true abundance fractions of materials. In this case, they can only be used for the purposes of material detection, discrimination, and classification, but not for material quantification. The authors present a fully constrained least squares (FCLS) linear spectral mixture analysis method for material quantification. Since no closed form can be derived for this method, an efficient algorithm is developed to yield optimal solutions. In order to further apply the designed algorithm to unknown image scenes, an unsupervised least squares error (LSE)-based method is also proposed to extend the FCLS method in an unsupervised manner.},
	number = {3},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Heinz, D. C. and {Chein-I-Chang}},
	month = mar,
	year = {2001},
	keywords = {hyperspectral imaging, Hyperspectral imaging, remote sensing, image processing, geophysical techniques, Algorithm design and analysis, least squares approximations, Hyperspectral sensors, Layout, Remote sensing, abundance fraction, algorithm, constraint, fully constrained least squares, geophysical measurement technique, geophysical signal processing, hyperspectral imagery, Image analysis, image colour analysis, Image processing, Least squares methods, linear spectral mixture analysis, material quantification, multidimensional signal processing, multispectral method, optical imaging, Pixel, Spectral analysis, unsupervised least squares error, visible region},
	pages = {529--545}
}

@misc{vervliet_tensorlab_2016,
	title = {Tensorlab 3.0},
	url = {https://www.tensorlab.net/},
	author = {Vervliet, N and Debals, O and Sorber, L and Van Barel, M and De Lathauwer, L},
	month = mar,
	year = {2016}
}

@misc{gerg_matlab_2008,
	title = {Matlab {Hyperspectral} {Toolbox}},
	url = {https://github.com/isaacgerg/matlabHyperspectralToolbox},
	abstract = {The open source Matlab Hyperspectral Toolbox is a matlab toolbox containing various hyperspectral exploitation algorithms. The toolbox is meant to be a concise repository of current state-of-the-art (2008) exploitation algorithms for learning and research purposes. The toolbox (will) include(s) functions for:},
	author = {Gerg, Isaac},
	year = {2008}
}

@article{zhu_spectral_2017,
	title = {Spectral {Unmixing} {Datasets} with {Ground} {Truths}},
	volume = {abs/1708.05125},
	url = {http://arxiv.org/abs/1708.05125},
	journal = {CoRR},
	author = {Zhu, Feiyun},
	year = {2017}
}

@article{qian_hyperspectral_2011,
	title = {Hyperspectral {Unmixing} via {L}$_{\textrm{1/2}}${Sparsity}-{Constrained} {Nonnegative} {Matrix} {Factorization}},
	volume = {49},
	doi = {10.1109/TGRS.2011.2144605},
	abstract = {Hyperspectral unmixing is a crucial preprocessing step for material classification and recognition. In the last decade, nonnegative matrix factorization (NMF) and its extensions have been intensively studied to unmix hyperspectral imagery and recover the material end-members. As an important constraint for NMF, sparsity has been modeled making use of theL1regularizer. Unfortunately, theL1regularizer cannot enforce further sparsity when the full additivity constraint of material abundances is used, hence limiting the practical efficacy of NMF methods in hyperspectral unmixing. In this paper, we extend the NMF method by incorporating theL1/2sparsity constraint, which we nameL1/2-NMF. TheL1/2regularizer not only induces sparsity but is also a better choice amongLq(0 {\textless};q{\textless}; 1) regularizers. We propose an iterative estimation algorithm forL1/2-NMF, which provides sparser and more accurate results than those delivered using theL1norm. We illustrate the utility of our method on synthetic and real hyperspectral data and compare our results to those yielded by other state-of-the-art methods.},
	number = {11},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Qian, Y. and Jia, S. and Zhou, J. and Robles-Kelly, A.},
	month = nov,
	year = {2011},
	keywords = {geophysical image processing, Hyperspectral imaging, remote sensing, matrix decomposition, Matrix decomposition, hyperspectral data, nonnegative matrix factorization, hyperspectral unmixing, Optimization, hyperspectral imagery, Pixel, $_{\textrm{1/2}}$regularizer, Convergence, Hyperspectral unmixing, L1/2regularizer, L1/2sparsity constrained NMF, material classification, material end members, material recognition, Materials, nonnegative matrix factorization (NMF), preprocessing step, sparse coding},
	pages = {4282--4297}
}

@inproceedings{nascimento_hyperspectral_2007,
	title = {Hyperspectral signal subspace estimation},
	doi = {10.1109/IGARSS.2007.4423531},
	abstract = {Given an hyperspectral image, the determination of the number of end members and the subspace where they live without any prior knowledge is crucial to the success of hyperspectral image analysis. This paper introduces a new minimum mean squared error based approach to infer the signal subspace in hyperspectral imagery. The method, termed hyperspectral signal identification by minimum error (HySime), is eigendecomposition based and it does not depend on any tuning parameters. It first estimates the signal and noise correlation matrices and then selects the subset of eigenvalues that best represents the signal subspace in the least squared error sense. The effectiveness of the proposed method is illustrated using simulated data based on U.S.G.S. laboratory spectra and real hyperspectral data collected by the AVIRIS sensor over Cuprite, Nevada.},
	booktitle = {2007 {IEEE} {International} {Geoscience} and {Remote} {Sensing} {Symposium}},
	author = {Nascimento, J. M. P. and Bioucas-Dias, J. M.},
	month = jul,
	year = {2007},
	note = {ISSN: 2153-7003},
	keywords = {Hyperspectral imaging, remote sensing, Principal component analysis, Hyperspectral sensors, Vectors, Signal processing, geophysical signal processing, Image analysis, AVIRIS sensor, Cuprite, eigendecomposition, Eigenvalues and eigenfunctions, Hybrid fiber coaxial cables, hyperspectral image analysis, hyperspectral signal identification by minimum error, hyperspectral signal subspace estimation, HySime, Infrared image sensors, Nevada, Telecommunications, USGS laboratory spectra},
	pages = {3225--3228}
}

@book{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	year = {2017},
	note = {\_eprint: 1412.6980}
}

@article{glorot_understanding_2010,
	title = {Understanding the difficulty of training deep feedforward neural networks},
	volume = {9},
	journal = {Journal of Machine Learning Research - Proceedings Track},
	author = {Glorot, Xavier and Bengio, Y.},
	year = {2010},
	pages = {249--256}
}

@article{imbiriba_low-rank_2020,
	title = {Low-{Rank} {Tensor} {Modeling} for {Hyperspectral} {Unmixing} {Accounting} for {Spectral} {Variability}},
	volume = {58},
	doi = {10.1109/TGRS.2019.2949543},
	number = {3},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Imbiriba, T. and Borsoi, R. A. and Bermudez, J. C. M.},
	year = {2020},
	pages = {1833--1842}
}

@article{xiong_hyperspectral_2019,
	title = {Hyperspectral {Unmixing} via {Total} {Variation} {Regularized} {Nonnegative} {Tensor} {Factorization}},
	volume = {57},
	doi = {10.1109/TGRS.2018.2872888},
	number = {4},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Xiong, F. and Qian, Y. and Zhou, J. and Tang, Y. Y.},
	year = {2019},
	pages = {2341--2357}
}

@article{sun_weighted_2020,
	title = {Weighted {Nonlocal} {Low}-{Rank} {Tensor} {Decomposition} {Method} for {Sparse} {Unmixing} of {Hyperspectral} {Images}},
	volume = {13},
	doi = {10.1109/JSTARS.2020.2980576},
	journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {Sun, L. and Wu, F. and Zhan, T. and Liu, W. and Wang, J. and Jeon, B.},
	year = {2020},
	pages = {1174--1188}
}
